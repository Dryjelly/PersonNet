{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 학습에 필요한 데이터를 만드는 함수 (MPN : 부정 이미지 쌍을 만들때 배수로 증가시켜주는 변수)\n",
    "def Image_Data(dir_path, MPN = 1):\n",
    "    image_list = os.listdir(dir_path)\n",
    "    image_list.sort()\n",
    "    \n",
    "    list_len = len(image_list)\n",
    "    print(\"이미지 파일 총 갯수: \",list_len)\n",
    "    \n",
    "    image1 = []\n",
    "    image2 = []\n",
    "    CoN = [] # correct[0 1] or not[0 1]\n",
    "    \n",
    "    #--- 묶음 제작\n",
    "    \n",
    "    pick_num = 10 # 뽑을 묶음 수\n",
    "    saved_name = image_list[0][:4] # ID가 이름의 4번째까지 이므로\n",
    "    front = 0\n",
    "    back = 0\n",
    "    \n",
    "    for index, img_name in enumerate(image_list):\n",
    "        if img_name[:4] == saved_name and index+1 != list_len:\n",
    "            continue\n",
    "        saved_name = img_name[:4]\n",
    "        back = index\n",
    "        if back-front == 1:\n",
    "            front = index\n",
    "            continue\n",
    "        for i in range(pick_num):\n",
    "            D1,D2 = random.sample(range(front,back),2)\n",
    "            image1.append(D1)\n",
    "            image2.append(D2)\n",
    "            CoN.append([0,1])\n",
    "        for i in range(pick_num*MPN):\n",
    "            D1 = random.sample(range(front,back),1)\n",
    "            while(True):\n",
    "                D2 = random.sample(range(0,list_len),1)\n",
    "                if front>D2[0] or D2[0]>back: break\n",
    "            image1.extend(D1)\n",
    "            image2.extend(D2)\n",
    "            CoN.append([1,0])\n",
    "        front = index\n",
    "        \n",
    "    CoN=np.asarray(CoN)\n",
    "    \n",
    "    nsp, sp = np.sum(CoN,axis=0)\n",
    "    print(\"Total: \",len(CoN))\n",
    "    print(\"같은 사람: \",sp)\n",
    "    print(\"다른 사람: \",nsp)\n",
    "    \n",
    "    return image_list, image1, image2, CoN\n",
    "\n",
    "### For Testing ###\n",
    "# testing_number 만큼의 사람들을 그 만큼 모두 짝지어 리턴해줍니다.\n",
    "\n",
    "# Rank 시스템으로 테스트 하기위해 데이터를 만들어주는 함수 (testing_number : 학습에 사용할 총 사람의 수)\n",
    "def ALL_Image_Data(dir_path, testing_number = 100):\n",
    "    image_list = os.listdir(dir_path)\n",
    "    image_list.sort()\n",
    "    \n",
    "    list_len = len(image_list)\n",
    "    print(\"이미지 파일 총 갯수: \",list_len)\n",
    "    print(\"뽑을 사람: \",testing_number)\n",
    "    \n",
    "    image1 = []\n",
    "    image2 = []\n",
    "    CoN = [] # correct[0 1] or not[0 1]\n",
    "    \n",
    "    \n",
    "    ### testing_number 만큼의 사람이 어디까진지 파악하는 코드 ###\n",
    "    saved_name = image_list[0][:4] # ID가 이름의 4번째까지 이므로\n",
    "    number_of_people = 1\n",
    "    \n",
    "    for index, img_name in enumerate(image_list):\n",
    "        if testing_number+1 == number_of_people: break\n",
    "        if img_name[:4] == saved_name:\n",
    "            continue\n",
    "        saved_name = img_name[:4]\n",
    "        number_of_people += 1\n",
    "    list_len = index-1\n",
    "    print(testing_number,\"명 만큼의 사람의 사진은\",list_len,\"장 입니다.\")\n",
    "    ### -------------------------------------------------- ###\n",
    "    \n",
    "    saved_name = image_list[0][:4]\n",
    "    front = 0\n",
    "    back = 0\n",
    "    \n",
    "    for index, img_name in enumerate(image_list[:list_len]):\n",
    "        \n",
    "        if img_name[:4] == saved_name and index+1 != list_len:\n",
    "            continue\n",
    "        if index+1 == list_len: index = list_len\n",
    "            \n",
    "        saved_name = img_name[:4]\n",
    "        back = index\n",
    "        \n",
    "        if back-front == 1:\n",
    "            front = index\n",
    "            continue\n",
    "        \n",
    "        image1.extend([front] * (list_len-1))\n",
    "        image2.extend(list(range(0,front))+list(range(front+1,list_len)))\n",
    "        for i in range(front):CoN.append([1,0])\n",
    "        for i in range(back-front-1): CoN.append([0,1])\n",
    "        for i in range(list_len-back): CoN.append([1,0])\n",
    "        \n",
    "        front = index\n",
    "    \n",
    "    CoN=np.asarray(CoN)\n",
    "    return image_list, image1, image2, CoN\n",
    "\n",
    "# 절대값 처리를 하지 않은 CND 함수\n",
    "def CrossND_ver1 (input1, input2, stride = 3):\n",
    "    # padding stride 미적용 일단 3으로 할 것\n",
    "    L = len(input1.shape)\n",
    "    if L==2: print(\"3차원 이상만 가능합니다.\")\n",
    "    elif L==3: paddings = tf.constant([[0, 0], [1, 1], [1, 1]])\n",
    "    elif L==4: paddings = tf.constant([[0, 0], [0, 0], [1, 1], [1, 1]])\n",
    "    else: print(\"미구현됨...\")\n",
    "        \n",
    "    input2 = tf.pad(input2, paddings, \"CONSTANT\")\n",
    "    for i in range(input1.shape[L-2]):\n",
    "        if L==3: F1 = tf.subtract(tf.reshape(input1[:,i,0],[input1.shape[L-3],1,1]),input2[:,i:i+stride,:stride])\n",
    "        elif L==4: F1 = tf.subtract(tf.reshape(input1[:,:,i,0],[-1,input1.shape[L-3],1,1]),input2[:,:,i:i+stride,:stride])\n",
    "        for j in range(input1.shape[L-1]):\n",
    "            if j == 0: continue\n",
    "            if L==3: F2 = tf.subtract(tf.reshape(input1[:,i,j],[input1.shape[L-3],1,1]),input2[:,i:i+stride,j:j+stride])\n",
    "            elif L==4: F2 = tf.subtract(tf.reshape(input1[:,:,i,j],[-1,input1.shape[L-3],1,1]),input2[:,:,i:i+stride,j:j+stride])\n",
    "            F1 = tf.concat([F1,F2],L-1)\n",
    "        if i==0:\n",
    "            output = F1\n",
    "            continue\n",
    "        output = tf.concat([output,F1],L-2)\n",
    "    return output\n",
    "\n",
    "# 절대값 처리를 한 CND 함수\n",
    "def CrossND_ver2 (input1, input2, stride = 3):\n",
    "    # padding stride 미적용 일단 3으로 할 것\n",
    "    L = len(input1.shape)\n",
    "    if L==2: print(\"3차원 이상만 가능합니다.\")\n",
    "    elif L==3: paddings = tf.constant([[0, 0], [1, 1], [1, 1]])\n",
    "    elif L==4: paddings = tf.constant([[0, 0], [0, 0], [1, 1], [1, 1]])\n",
    "    else: print(\"미구현됨...\")\n",
    "        \n",
    "    input2 = tf.pad(input2, paddings, \"CONSTANT\")\n",
    "    for i in range(input1.shape[L-2]):\n",
    "        if L==3: F1 = tf.subtract(tf.reshape(input1[:,i,0],[input1.shape[L-3],1,1]),input2[:,i:i+stride,:stride])\n",
    "        elif L==4: F1 = tf.subtract(tf.reshape(input1[:,:,i,0],[-1,input1.shape[L-3],1,1]),input2[:,:,i:i+stride,:stride])\n",
    "        F1 = tf.abs(F1)\n",
    "        for j in range(input1.shape[L-1]):\n",
    "            if j == 0: continue\n",
    "            if L==3: F2 = tf.subtract(tf.reshape(input1[:,i,j],[input1.shape[L-3],1,1]),input2[:,i:i+stride,j:j+stride])\n",
    "            elif L==4: F2 = tf.subtract(tf.reshape(input1[:,:,i,j],[-1,input1.shape[L-3],1,1]),input2[:,:,i:i+stride,j:j+stride])\n",
    "            F2 = tf.abs(F2)\n",
    "            F1 = tf.concat([F1,F2],L-1)\n",
    "        if i==0:\n",
    "            output = F1\n",
    "            continue\n",
    "        output = tf.concat([output,F1],L-2)\n",
    "    return output\n",
    "\n",
    "# 추가적인 거리 측정 방식을 도입한 CND 함수 (실패함)\n",
    "def CrossND_ver3 (input1, input2, stride = 3):\n",
    "    # padding stride 미적용 일단 3으로 할 것\n",
    "    L = len(input1.shape)\n",
    "    if L==2: print(\"3차원 이상만 가능합니다.\")\n",
    "    elif L==3: paddings = tf.constant([[0, 0], [1, 1], [1, 1]])\n",
    "    elif L==4: paddings = tf.constant([[0, 0], [0, 0], [1, 1], [1, 1]])\n",
    "    else: print(\"미구현됨...\")\n",
    "        \n",
    "    input2 = tf.pad(input2, paddings, \"CONSTANT\")\n",
    "    for i in range(input1.shape[L-2]):\n",
    "        if L==3: F1 = tf.subtract(tf.reshape(input1[:,i,0],[input1.shape[L-3],1,1]),input2[:,i:i+stride,:stride])\n",
    "        elif L==4: F1 = tf.subtract(tf.reshape(input1[:,:,i,0],[-1,input1.shape[L-3],1,1]),input2[:,:,i:i+stride,:stride])\n",
    "        F1 = tf.square(F1)\n",
    "        for j in range(input1.shape[L-1]):\n",
    "            if j == 0: continue\n",
    "            if L==3: F2 = tf.subtract(tf.reshape(input1[:,i,j],[input1.shape[L-3],1,1]),input2[:,i:i+stride,j:j+stride])\n",
    "            elif L==4: F2 = tf.subtract(tf.reshape(input1[:,:,i,j],[-1,input1.shape[L-3],1,1]),input2[:,:,i:i+stride,j:j+stride])\n",
    "            F2 = tf.square(F2)\n",
    "            F1 = tf.concat([F1,F2],L-1)\n",
    "        if i==0:\n",
    "            output = F1\n",
    "            continue\n",
    "        output = tf.concat([output,F1],L-2)\n",
    "    return output\n",
    "\n",
    "# 초기 CNN 계층 (지금은 쓰이지 않음)\n",
    "def ConvNet_ori(x, in_channels, filter_count, stride = 1,filter_size = 3, mu = 0.5, sigma = 0.01, padding = 'SAME'):\n",
    "    W = tf.get_variable(\"W\",[filter_size,filter_size,in_channels,filter_count],initializer=tf.random_normal_initializer(0, sigma))\n",
    "    b = tf.get_variable(\"b\",[1,1,filter_count],initializer=tf.random_normal_initializer(mu, sigma))\n",
    "    x = tf.nn.conv2d(x,W,strides=[1,stride,stride,1],padding=padding)\n",
    "    x = tf.add(x,b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# CNN 계층\n",
    "def ConvNet(x, in_channels, filter_count, batch_prob, stride = 1,filter_size = 3, mu = 0.5, sigma = 0.01, padding = 'SAME'):\n",
    "    # W = tf.Variable(tf.random_normal([filter_size,filter_size,in_channels,filter_count], stddev = sigma))\n",
    "    # b = tf.Variable(tf.random_normal([1,1,filter_count], mean = mu, stddev = sigma))\n",
    "    W = tf.get_variable(\"W\",[filter_size,filter_size,in_channels,filter_count],initializer=tf.random_normal_initializer(0, sigma))\n",
    "    #print(W)\n",
    "    #b = tf.get_variable(\"b\",[1,1,filter_count],initializer=tf.random_normal_initializer(mu, sigma))\n",
    "    x = tf.nn.conv2d(x,W,strides=[1,stride,stride,1],padding=padding)\n",
    "    #x = tf.add(x,b)\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob, name = \"batch_norm\")\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# inception 계층\n",
    "def inception2d_v2(x, in_channels, filter_count, batch_prob, mu=0, sigma=0.01):\n",
    "    \n",
    "    # 1x1\n",
    "    one_filter_1 = tf.get_variable(\"one_filter_1\",[1, 1, in_channels, filter_count[0]],initializer=tf.initializers.truncated_normal(mu,sigma))\n",
    "    one_by_one = tf.nn.conv2d(x, one_filter_1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    one_by_one = tf.layers.batch_normalization(one_by_one, center=True, scale=True, training=batch_prob, name = \"batch_norm1\")\n",
    "    \n",
    "    # 3x3\n",
    "    one_filter_3 = tf.get_variable(\"one_filter_3\",[1, 1, in_channels, int(filter_count[1]*(2/3))],initializer=tf.initializers.truncated_normal(mu,sigma))\n",
    "    three_filter = tf.get_variable(\"three_filter\",[3, 3, int(filter_count[1]*(2/3)), filter_count[1]],initializer=tf.initializers.truncated_normal(mu,sigma))\n",
    "    three_by_three = tf.nn.conv2d(x, one_filter_3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    three_by_three = tf.nn.conv2d(three_by_three, three_filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    three_by_three = tf.layers.batch_normalization(three_by_three, center=True, scale=True, training=batch_prob, name = \"batch_norm2\")\n",
    "    \n",
    "    # 5x5\n",
    "    one_filter_5 = tf.get_variable(\"one_filter_5\",[1, 1, in_channels, int(filter_count[2]/2)],initializer=tf.initializers.truncated_normal(mu,sigma))\n",
    "    five_filter = tf.get_variable(\"five_filter\",[5, 5, int(filter_count[2]/2), filter_count[2]],initializer=tf.initializers.truncated_normal(mu,sigma))\n",
    "    five_by_five = tf.nn.conv2d(x, one_filter_5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    five_by_five = tf.nn.conv2d(five_by_five, five_filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    five_by_five = tf.layers.batch_normalization(five_by_five, center=True, scale=True, training=batch_prob, name = \"batch_norm3\")\n",
    "    \n",
    "    # avg pooling\n",
    "    one_filter_p = tf.get_variable(\"one_filter_p\",[1, 1, in_channels, filter_count[3]],initializer=tf.initializers.truncated_normal(mu,sigma))\n",
    "    pooling = tf.nn.avg_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    pooling_by_one = tf.nn.conv2d(pooling, one_filter_p, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    x = tf.concat([one_by_one, three_by_three, five_by_five, pooling_by_one], axis=3)  # Concat in the 4th dim to stack\n",
    "    \n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# PCB계층(비슷한 영역을 찾아내는 계층)의 추가 (실패로 인해 쓰이지 않음)\n",
    "def PCB(x, stripes):\n",
    "    M = x.shape[1]\n",
    "    N = x.shape[2]\n",
    "    T = x.shape[3]\n",
    "    \n",
    "    W = tf.get_variable(\"PCB_W\",[1,1,T,stripes],initializer=tf.random_normal_initializer)\n",
    "    \n",
    "    Pi_f = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    Pi_f = tf.nn.softmax(Pi_f)\n",
    "    \n",
    "    new_x = x * tf.reshape(Pi_f[:,:,:,0],[-1,M,N,1])\n",
    "    for i in range(1,stripes):\n",
    "        new_x = tf.concat([new_x, x * tf.reshape(Pi_f[:,:,:,i],[-1,M,N,1])], axis=3)\n",
    "    \n",
    "    return new_x\n",
    "\n",
    "def PCB_v2(x, stripes, RPP = True):\n",
    "    M = x.shape[1]\n",
    "    N = x.shape[2]\n",
    "    T = x.shape[3]\n",
    "    \n",
    "    if RPP:\n",
    "        W = tf.get_variable(\"PCB_W\",[1,1,T,stripes],initializer=tf.random_normal_initializer)\n",
    "    \n",
    "        Pi_f = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        Pi_f = tf.nn.softmax(Pi_f)\n",
    "    \n",
    "        new_x = x * tf.reshape(Pi_f[:,:,:,0],[-1,M,N,1])\n",
    "        for i in range(1,stripes):\n",
    "            new_x = tf.concat([new_x, x * tf.reshape(Pi_f[:,:,:,i],[-1,M,N,1])], axis=3)\n",
    "        new_x = tf.nn.relu(new_x)\n",
    "    else:\n",
    "        s = int(int(M)/stripes)\n",
    "        new_x = x[:,:s,:,:]\n",
    "        for i in range(1,stripes):\n",
    "            new_x = tf.concat([new_x,x[:,i*s:(i+1)*s,:,:]], axis=3)\n",
    "    \n",
    "    return new_x\n",
    "\n",
    "def PCB_v3(x, stripes, batch_prob,RPP = True):\n",
    "    M = x.shape[1]\n",
    "    N = x.shape[2]\n",
    "    T = x.shape[3]\n",
    "    \n",
    "    s = int(int(M)/stripes)\n",
    "    \n",
    "    if RPP:\n",
    "        W = tf.get_variable(\"PCB_W\",[1,1,T,stripes],initializer=tf.random_normal_initializer)\n",
    "    \n",
    "        Pi_f = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        Pi_f = tf.layers.batch_normalization(Pi_f, center=True, scale=True, training=batch_prob, name = \"batch_norm_1\")\n",
    "        Pi_f = tf.nn.softmax(Pi_f)\n",
    "    \n",
    "        new_x = x[:,:s,:,:] * tf.reshape(Pi_f[:,:s,:,0],[-1,s,N,1])\n",
    "        for i in range(1,stripes):\n",
    "            new_x = tf.concat([new_x, x[:,i*s:(i+1)*s,:,:] * tf.reshape(Pi_f[:,i*s:(i+1)*s,:,i],[-1,s,N,1])], axis=3)\n",
    "        \n",
    "        new_x = tf.layers.batch_normalization(new_x, center=True, scale=True, training=batch_prob, name = \"batch_norm_2\")\n",
    "        new_x = tf.nn.relu(new_x)\n",
    "    else:\n",
    "        new_x = x[:,:s,:,:]\n",
    "        for i in range(1,stripes):\n",
    "            new_x = tf.concat([new_x,x[:,i*s:(i+1)*s,:,:]], axis=3)\n",
    "    \n",
    "    return new_x\n",
    "\n",
    "def PCB_v4(x1, x2, y, batch_prob):\n",
    "    M = x1.shape[1]\n",
    "    N = x1.shape[2]\n",
    "    T = x1.shape[3]\n",
    "    \n",
    "    new_x1 = tf.reduce_mean(x1, axis = 2)\n",
    "    new_x1 = tf.reduce_mean(new_x1, axis = 1)\n",
    "    new_x1 = tf.reshape(new_x1, [-1,T])\n",
    "    \n",
    "    new_x2 = tf.reduce_mean(x2, axis = 2)\n",
    "    new_x2 = tf.reduce_mean(new_x2, axis = 1)\n",
    "    new_x2 = tf.reshape(new_x2, [-1,T])\n",
    "    \n",
    "    with tf.variable_scope(\"PCB\") as scope:\n",
    "        W = tf.get_variable(\"W\",[T,128],initializer=tf.random_normal_initializer(0,0.2))\n",
    "        b = tf.get_variable(\"b\",[128],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "        \n",
    "        new_x1 = tf.add(tf.matmul(new_x1,W),b)\n",
    "        new_x1 = tf.layers.batch_normalization(new_x1, center=True, scale=True, training=batch_prob, name = \"batch_norm\")\n",
    "        new_x1 = tf.nn.sigmoid(new_x1)\n",
    "        scope.reuse_variables()\n",
    "        new_x2 = tf.add(tf.matmul(new_x2,W),b)\n",
    "        new_x2 = tf.layers.batch_normalization(new_x2, center=True, scale=True, training=batch_prob, name = \"batch_norm\")\n",
    "        new_x2 = tf.nn.sigmoid(new_x2)\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.square(new_x1 - new_x2), axis = 1)\n",
    "        cost = tf.reduce_mean(y[:,1] * cost - y[:,0] * tf.log(cost))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def PCB_FC_part(x, batch_prob, output = 1):\n",
    "    T = x.shape[-1]\n",
    "    if len(x.shape) > 2:\n",
    "        T = T * x.shape[1] * x.shape[2]\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\",[T,100],initializer=tf.random_normal_initializer(0,0.2))\n",
    "    b1 = tf.get_variable(\"b1\",[100],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "    W2 = tf.get_variable(\"W2\",[100,output],initializer=tf.random_normal_initializer(0,0.2))\n",
    "    b2 = tf.get_variable(\"b2\",[output],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "    \n",
    "    x = tf.reshape(x, [-1,T])\n",
    "    x = tf.add(tf.matmul(x,W1),b1)\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob, name = \"batch_norm1\")\n",
    "    x = tf.nn.sigmoid(x)\n",
    "    x = tf.add(tf.matmul(x,W2),b2)\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob, name = \"batch_norm2\")\n",
    "    #x = tf.nn.sigmoid(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def PCB_v5(new_x, x2, batch_prob):\n",
    "    M = x2.shape[1]\n",
    "    N = x2.shape[2]\n",
    "    T = x2.shape[3]\n",
    "    \n",
    "    with tf.variable_scope(\"PCB\") as scope:\n",
    "        new_x2 = tf.reshape(x2[:,0,0,:] * PCB_FC_part(new_x[:,0,0,:],batch_prob),[-1,1,1,T])\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                if i==0 and j ==0: continue\n",
    "                scope.reuse_variables()\n",
    "                part = tf.reshape(x2[:,i,j,:] * PCB_FC_part(new_x[:,i,j,:],batch_prob),[-1,1,1,T])\n",
    "                new_x2 = tf.concat([new_x2, part], axis = 1)\n",
    "        new_x2 = tf.reshape(new_x2,[-1,M,N,T])\n",
    "        \n",
    "    return new_x2\n",
    "    \n",
    "# 단순하게 특징점들을 3부분으로 나누는 함수\n",
    "def Seperator(x, stripes):\n",
    "    M = int(x.shape[1])\n",
    "    T = int(x.shape[3])\n",
    "    Part = []\n",
    "    \n",
    "    for i in range(stripes):\n",
    "        s = int(T/stripes)\n",
    "        chunk = x[:,:,:,i*s:(i+1)*s]\n",
    "        chunk = tf.reduce_mean(chunk, axis = 2)#, keepdims=True)\n",
    "        chunk = tf.reduce_mean(chunk, axis = 1)#, keepdims=True)\n",
    "        Part.append(chunk)\n",
    "            \n",
    "    return Part\n",
    "\n",
    "# 3부분으로 나뉜 특징점에 특정 가중치를 곱하여 내보내는 함수\n",
    "def Seperator_v2(x):\n",
    "    M = int(x.shape[1])\n",
    "    s = int(M/3)\n",
    "    \n",
    "    new_x = x[:,:s,:,:] * 1.25\n",
    "    new_x = tf.concat([new_x, x[:,s:2*s,:,:] * 1.0], axis = 1)\n",
    "    new_x = tf.concat([new_x, x[:,2*s:,:,:] * 0.75], axis = 1)\n",
    "    \n",
    "    return tf.nn.relu(new_x)\n",
    "\n",
    "# 나누는 과정을 편하게 하기위해 더 많은 기능을 추가하였다. (w : 추가할 가중치 리스트, part : 나눌 횟수)\n",
    "def Seperator_v2_1(x,w = [1.5, 1.25, 1.0, 0.75, 0.5],part = 5):\n",
    "    M = int(x.shape[1])\n",
    "    s = int(M/part)\n",
    "    \n",
    "    new_x = x[:,:s,:,:] * w[0]\n",
    "    for i in range(1,part-1):\n",
    "        new_x = tf.concat([new_x, x[:,i*s:(i+1)*s,:,:] * w[i]], axis = 1)\n",
    "    new_x = tf.concat([new_x, x[:,(part-1)*s:,:,:] * w[part-1]], axis = 1)\n",
    "    \n",
    "    return tf.nn.relu(new_x)\n",
    "\n",
    "# 신경망이 최적의 가중치를 찾아내길 원했으나 실패함\n",
    "def Seperator_v3(x):\n",
    "    M = int(x.shape[1])\n",
    "    s = int(M/3)\n",
    "    \n",
    "    b1 = tf.get_variable(\"b1\",[1],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "    b2 = tf.get_variable(\"b2\",[1],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "    b3 = tf.get_variable(\"b3\",[1],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "    \n",
    "    new_x = x[:,:s,:,:] * b1\n",
    "    new_x = tf.concat([new_x, x[:,s:2*s,:,:] * b2], axis = 1)\n",
    "    new_x = tf.concat([new_x, x[:,2*s:,:,:] * b3], axis = 1)\n",
    "    \n",
    "    return new_x\n",
    "\n",
    "# 각 이미지들이 서로의 특징점에 영향을 받아 최적의 가중치를 찾는 함수 (실패함)\n",
    "def Seperator_v4(x1, x2, part, batch_prob):\n",
    "    M = int(x1.shape[1])\n",
    "    s = int(M/part)\n",
    "    r = int(M%part)\n",
    "    \n",
    "    dist_x = tf.abs(x1 - x2)\n",
    "    dist_x = tf.reduce_mean(dist_x, axis=[1,2])\n",
    "    prob = PCB_FC_part(dist_x, batch_prob, output = part)\n",
    "    prob = tf.nn.softmax(prob) * part\n",
    "    \n",
    "    output = tf.reduce_mean(prob, axis = 0)\n",
    "    \n",
    "    new_x1 = x1[:,:s,:,:] * prob[:,None,None,None,0]\n",
    "    new_x2 = x2[:,:s,:,:] * prob[:,None,None,None,0]\n",
    "    for i in range(1,part):\n",
    "        new_x1 = tf.concat([new_x1, x1[:,i*s:(i+1)*s,:,:] * prob[:,None,None,None,i]], axis = 1)\n",
    "        new_x2 = tf.concat([new_x2, x2[:,i*s:(i+1)*s,:,:] * prob[:,None,None,None,i]], axis = 1)\n",
    "    new_x1 = tf.concat([new_x1, x1[:,part*s:,:,:] * prob[:,None,None,None,part-1]], axis = 1)\n",
    "    new_x2 = tf.concat([new_x2, x2[:,part*s:,:,:] * prob[:,None,None,None,part-1]], axis = 1)\n",
    "    \n",
    "    return tf.nn.relu(new_x1), tf.nn.relu(new_x2), output\n",
    "\n",
    "# 위의 함수에서 좀 더 발전된 형태\n",
    "def Seperator_v5(x1, x2, part, batch_prob):\n",
    "    M = int(x1.shape[1])\n",
    "    s = int(M/part)\n",
    "    r = int(M%part)\n",
    "    \n",
    "    dist_x = tf.abs(x1 - x2)\n",
    "    \n",
    "    for i in range(s):\n",
    "        dist_x_part = tf.reduce_mean(dist_x[:,i*s:(i+1)*s,:,:], axis=[1,2])\n",
    "        prob = PCB_FC_part(dist_x_part, batch_prob, output = 1)\n",
    "        prob = tf.nn.sigmoid(prob) * part\n",
    "    \n",
    "    new_x1 = x1[:,:s,:,:] * prob[:,None,None,None,0]\n",
    "    new_x2 = x2[:,:s,:,:] * prob[:,None,None,None,0]\n",
    "    for i in range(1,part):\n",
    "        new_x1 = tf.concat([new_x1, x1[:,i*s:(i+1)*s,:,:] * prob[:,None,None,None,i]], axis = 1)\n",
    "        new_x2 = tf.concat([new_x2, x2[:,i*s:(i+1)*s,:,:] * prob[:,None,None,None,i]], axis = 1)\n",
    "    new_x1 = tf.concat([new_x1, x1[:,part*s:,:,:] * prob[:,None,None,None,part-1]], axis = 1)\n",
    "    new_x2 = tf.concat([new_x2, x2[:,part*s:,:,:] * prob[:,None,None,None,part-1]], axis = 1)\n",
    "    \n",
    "    return new_x1, new_x2\n",
    "\n",
    "# 4계층으로 이루어진 완전 연결 계층\n",
    "def FC_layer_ori(x, input_size, output_size = 2):\n",
    "    W1 = tf.Variable(tf.random_normal([input_size,4096], stddev = 0.2))\n",
    "    W2 = tf.Variable(tf.random_normal([4096,4096], stddev = 0.2))\n",
    "    W3 = tf.Variable(tf.random_normal([4096,512], stddev = 0.2))\n",
    "    W4 = tf.Variable(tf.random_normal([512,output_size], stddev = 0.2))\n",
    "\n",
    "    b1 = tf.Variable(tf.random_normal([4096], mean = 0.5, stddev = 0.01))\n",
    "    b2 = tf.Variable(tf.random_normal([4096], mean = 0.5, stddev = 0.01))\n",
    "    b3 = tf.Variable(tf.random_normal([512], mean = 0.5, stddev = 0.01))\n",
    "    b4 = tf.Variable(tf.random_normal([output_size], mean = 0.5, stddev = 0.01))\n",
    "    \n",
    "    x = tf.reshape(x, [-1,input_size])\n",
    "    x = tf.add(tf.matmul(x,W1),b1)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "\n",
    "    x = tf.add(tf.matmul(x,W2),b2)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "\n",
    "    x = tf.add(tf.matmul(x,W3),b3)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "\n",
    "    x = tf.add(tf.matmul(x,W4),b4)\n",
    "    return x\n",
    "\n",
    "# 4계층으로 이루어진 완전 연결 계층\n",
    "def FC_layer(x, input_size, batch_prob, output_size = 2):\n",
    "    W1 = tf.Variable(tf.random_normal([input_size,4096], stddev = 0.2))\n",
    "    W2 = tf.Variable(tf.random_normal([4096,4096], stddev = 0.2))\n",
    "    W3 = tf.Variable(tf.random_normal([4096,512], stddev = 0.2))\n",
    "    W4 = tf.Variable(tf.random_normal([512,output_size], stddev = 0.2))\n",
    "\n",
    "    b1 = tf.Variable(tf.random_normal([4096], mean = 0.5, stddev = 0.01))\n",
    "    b2 = tf.Variable(tf.random_normal([4096], mean = 0.5, stddev = 0.01))\n",
    "    b3 = tf.Variable(tf.random_normal([512], mean = 0.5, stddev = 0.01))\n",
    "    b4 = tf.Variable(tf.random_normal([output_size], mean = 0.5, stddev = 0.01))\n",
    "    \n",
    "    x = tf.reshape(x, [-1,input_size])\n",
    "    x = tf.add(tf.matmul(x,W1),b1)\n",
    "\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "    #print(x)\n",
    "\n",
    "    x = tf.add(tf.matmul(x,W2),b2)\n",
    "\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "    #print(x)\n",
    "\n",
    "    x = tf.add(tf.matmul(x,W3),b3)\n",
    "\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "    #print(x)\n",
    "\n",
    "    x = tf.add(tf.matmul(x,W4),b4)\n",
    "    #print(x)\n",
    "    return x\n",
    "\n",
    "# 재사용을 위해 만든 완전연결 계층\n",
    "def FC_layer_v2(x, in_channels, batch_prob, output_size = 2):\n",
    "    # W1 = tf.Variable(tf.random_normal([in_channels,512], stddev = 0.2))\n",
    "    W1 = tf.get_variable(\"W1\",[in_channels,512],initializer=tf.random_normal_initializer(0,0.2))\n",
    "    # W2 = tf.Variable(tf.random_normal([512,512], stddev = 0.2))\n",
    "    W2 = tf.get_variable(\"W2\",[512,512],initializer=tf.random_normal_initializer(0,0.2))\n",
    "    # W3 = tf.Variable(tf.random_normal([512,output_size], stddev = 0.2))\n",
    "    W3 = tf.get_variable(\"W3\",[512,output_size],initializer=tf.random_normal_initializer(0,0.2))\n",
    "\n",
    "    # b1 = tf.Variable(tf.random_normal([512], mean = 0.5, stddev = 0.01))\n",
    "    b1 = tf.get_variable(\"b1\",[512],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "    #b2 = tf.Variable(tf.random_normal([512], mean = 0.5, stddev = 0.01))\n",
    "    b2 = tf.get_variable(\"b2\",[512],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "    #b3 = tf.Variable(tf.random_normal([output_size], mean = 0.5, stddev = 0.01))\n",
    "    b3 = tf.get_variable(\"b3\",[output_size],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "    \n",
    "    x = tf.reshape(x, [-1,in_channels])\n",
    "    x = tf.add(tf.matmul(x,W1),b1)\n",
    "\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "    #print(x)\n",
    "\n",
    "    x = tf.add(tf.matmul(x,W2),b2)\n",
    "\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "    #print(x)\n",
    "\n",
    "    x = tf.add(tf.matmul(x,W3),b3)\n",
    "\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob)\n",
    "    #print(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# 1계층의 완전 연결계층\n",
    "def FC_layer_v3(x, in_channels, batch_prob, output_size = 2):\n",
    "    W1 = tf.get_variable(\"W\",[in_channels,output_size],initializer=tf.random_normal_initializer(0,0.2))\n",
    "    b1 = tf.get_variable(\"b\",[output_size],initializer=tf.random_normal_initializer(0.5,0.01))\n",
    "    \n",
    "    x = tf.reshape(x, [-1,in_channels])\n",
    "    x = tf.add(tf.matmul(x,W1),b1)\n",
    "\n",
    "    x = tf.layers.batch_normalization(x, center=True, scale=True, training=batch_prob)\n",
    "    return x\n",
    "\n",
    "# 재 학습시 초기화 되지 않은 변수들을 초기화 시켜주는 함수\n",
    "def initialize_uninitialized(sess):\n",
    "    global_vars          = tf.global_variables()\n",
    "    is_not_initialized   = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    print([str(i.name) for i in not_initialized_vars]) # only for testing\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터를 뽑습니다. 경로를 잘 확인해 주세요\n",
    "\n",
    "train_dir = '/home/esdl/tensorflow/cuhk03_release/images/detected'\n",
    "# train_dir = '/home/esdl/tensorflow/Market-1501-v15.09.15/bounding_box_train'\n",
    "train_list, train_data1, train_data2, train_CoN = Image_Data(train_dir,5)\n",
    "\n",
    "data_index = list(range(len(train_data1)))\n",
    "\n",
    "for i in data_index[:10]:\n",
    "    img1 = cv2.imread(train_dir+'/'+train_list[train_data1[i]],cv2.IMREAD_COLOR)\n",
    "    img2 = cv2.imread(train_dir+'/'+train_list[train_data2[i]],cv2.IMREAD_COLOR)\n",
    "    fig , ax = plt.subplots(1, 2)\n",
    "    ax[0].set_axis_off()\n",
    "    ax[1].set_axis_off()\n",
    "\n",
    "    ax[0].imshow(img1)\n",
    "    ax[1].imshow(img2)\n",
    "    \n",
    "    print(\"first pic = \",train_list[train_data1[i]])\n",
    "    print(\"second pic = \",train_list[train_data2[i]])\n",
    "    print(\"answer is = \",train_CoN[i])\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 뽑습니다. 경로를 잘 확이해 주세요\n",
    "\n",
    "test_dir = '/home/esdl/tensorflow/campus'\n",
    "test_list, test_data1, test_data2, test_CoN = Image_Data(test_dir)\n",
    "\n",
    "test_data_index = list(range(len(test_data1)))\n",
    "random.shuffle(test_data_index)\n",
    "\n",
    "for i in test_data_index[:10]:\n",
    "    img1 = cv2.imread(test_dir+'/'+test_list[test_data1[i]],cv2.IMREAD_COLOR)\n",
    "    img2 = cv2.imread(test_dir+'/'+test_list[test_data2[i]],cv2.IMREAD_COLOR)\n",
    "    fig , ax = plt.subplots(1, 2)\n",
    "    ax[0].set_axis_off()\n",
    "    ax[1].set_axis_off()\n",
    "\n",
    "    ax[0].imshow(img1)\n",
    "    ax[1].imshow(img2)\n",
    "    \n",
    "    print(\"first pic = \",test_list[test_data1[i]])\n",
    "    print(\"second pic = \",test_list[test_data2[i]])\n",
    "    print(\"answer is = \",test_CoN[i])\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용될 모델의 구조입니다.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "data_x_size = 160\n",
    "data_y_size = 60\n",
    "\n",
    "batch_prob = tf.placeholder(tf.bool, name=\"batch_prob\")\n",
    "\n",
    "X1 = tf.placeholder(tf.float32, [None, data_x_size, data_y_size, 3], name=\"input1\") # input image 1\n",
    "X2 = tf.placeholder(tf.float32, [None, data_x_size, data_y_size, 3], name=\"input2\") # input image 2\n",
    "Y = tf.placeholder(tf.float32, [None, 2]) # output : different = [1 0] / same = [0 1]\n",
    "print(Y)\n",
    "#----------------------------------------------------- conv net\n",
    "with tf.variable_scope(\"image_filter_1\") as scope:\n",
    "    L1_1 = ConvNet(X1, 3, 32, batch_prob)\n",
    "    scope.reuse_variables()\n",
    "    L1_2 = ConvNet(X2, 3, 32, batch_prob)\n",
    "    L1_1 = tf.nn.max_pool(L1_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    L1_2 = tf.nn.max_pool(L1_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    print(L1_1,'\\n',L1_2)\n",
    "\n",
    "with tf.variable_scope(\"image_filter_2\") as scope:\n",
    "    L2_1 = inception2d_v2(L1_1, 32, [10, 70, 4, 4], batch_prob)\n",
    "    scope.reuse_variables()\n",
    "    L2_2 = inception2d_v2(L1_2, 32, [10, 70, 4, 4], batch_prob)\n",
    "    L2_1 = tf.nn.max_pool(L2_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    L2_2 = tf.nn.max_pool(L2_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    print(L2_1,'\\n',L2_2)\n",
    "\n",
    "with tf.variable_scope(\"image_filter_3\") as scope:\n",
    "    L3_1 = inception2d_v2(L2_1, 88, [20, 90, 8, 8], batch_prob)\n",
    "    scope.reuse_variables()\n",
    "    L3_2 = inception2d_v2(L2_2, 88, [20, 90, 8, 8], batch_prob)\n",
    "    L3_1 = tf.nn.max_pool(L3_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    L3_2 = tf.nn.max_pool(L3_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "print(L3_1,'\\n',L3_2)\n",
    "\n",
    "with tf.variable_scope(\"image_filter_4\") as scope:\n",
    "    L4_1 = inception2d_v2(L3_1, 126, [30, 100, 12, 12], batch_prob)\n",
    "    scope.reuse_variables()\n",
    "    L4_2 = inception2d_v2(L3_2, 126, [30, 100, 12, 12], batch_prob)\n",
    "    L4_1 = tf.nn.max_pool(L4_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    L4_2 = tf.nn.max_pool(L4_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    print(L4_1,'\\n',L4_2)\n",
    "\n",
    "with tf.variable_scope(\"SP\") as scope:\n",
    "    L4_1 = Seperator_v2_1(L4_1,[0.5, 0.75, 1.0, 1.25, 1.5],5)\n",
    "    L4_2 = Seperator_v2_1(L4_2,[0.5, 0.75, 1.0, 1.25, 1.5],5)\n",
    "    print(L4_1,'\\n',L4_2)\n",
    "\n",
    "#----------------------------------------------------- cross neighborhood difference\n",
    "with tf.variable_scope(\"CND\") as scope:\n",
    "    L4_1 = tf.transpose(L4_1,[0,3,1,2])\n",
    "    L4_2 = tf.transpose(L4_2,[0,3,1,2])\n",
    "    L4 = CrossND_ver2(L4_1,L4_2)\n",
    "    L4 = tf.transpose(L4,[0,2,3,1])\n",
    "    L4 = tf.nn.relu(L4)\n",
    "    print(L4)\n",
    "\n",
    "#----------------------------------------------------- conv net\n",
    "with tf.variable_scope(\"Conv_Net_1\") as scope:\n",
    "    L5 = ConvNet(L4, 154, 154, batch_prob, stride = 3)\n",
    "    #L5 = tf.nn.avg_pool(L5, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    print(L5)\n",
    "\n",
    "#----------------------------------------------------- conv net\n",
    "with tf.variable_scope(\"Conv_Net_2\") as scope:\n",
    "    L5 = ConvNet(L5, 154, 154, batch_prob)\n",
    "    L5 = tf.nn.avg_pool(L5, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    print(L5)\n",
    "    \n",
    "#----------------------------------------------------- dense\n",
    "with tf.variable_scope(\"Dense\") as scope:\n",
    "    model = FC_layer_v2(L5, 5*2*154, batch_prob)\n",
    "    model = tf.identity(model, \"model\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 하는 과정입니다.\n",
    "# SAVER_DIR 위치에 모델과 파라미터가 저장됩니다.\n",
    "# 학습은 총 32번의 에폭으로 진행되며 수정하실 수 있습니다.\n",
    "\n",
    "batch_size=100\n",
    "total_batch=int(len(train_data1) / batch_size)\n",
    "print(\"전체 데이터 크기는\",len(train_data1),\"배치 크기는\",batch_size,\"전체 배치는\",total_batch)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "#global_step = tf.Variable(0, trainable=False)\n",
    "#starter_learning_rate = 0.00006\n",
    "#learning_rate = starter_learning_rate\n",
    "#learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                           total_batch, 0.99, staircase=True)\n",
    "#optimizer = tf.train.AdamOptimizer(0.00006).minimize(cost)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.00006).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.00006).minimize(cost)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.00006).minimize(cost)\n",
    "\n",
    "# tf.train.Saver를 이용해서 모델과 파라미터를 저장합니다.\n",
    "SAVER_DIR = \"Re_Id_TEST/New/Add_conv2\"\n",
    "saver = tf.train.Saver()\n",
    "checkpoint_path = os.path.join(SAVER_DIR, \"model\")\n",
    "ckpt = tf.train.get_checkpoint_state(SAVER_DIR)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(init)\n",
    "\n",
    "# 에폭을 조정하려면 32를 원하는 숫자로 바꿔주세요\n",
    "for epoch in range(32):\n",
    "    total_cost=0\n",
    "    index_num=0\n",
    "    for i in range(total_batch):\n",
    "        \n",
    "        input_data1 = np.empty((batch_size, data_x_size, data_y_size, 3))\n",
    "        input_data2 = np.empty((batch_size, data_x_size, data_y_size, 3))\n",
    "        batch_i=0\n",
    "        for index_num in data_index[i*batch_size:(i+1)*batch_size]:\n",
    "            input_data1[batch_i,:,:,:] = cv2.resize(cv2.imread(train_dir+'/'+train_list[train_data1[index_num]],cv2.IMREAD_COLOR),(data_y_size,data_x_size))\n",
    "            input_data2[batch_i,:,:,:] = cv2.resize(cv2.imread(train_dir+'/'+train_list[train_data2[index_num]],cv2.IMREAD_COLOR),(data_y_size,data_x_size))\n",
    "            batch_i = batch_i+1\n",
    "        batch_ys = train_CoN[i*batch_size : (i+1)*batch_size]\n",
    "        _, cost_val=sess.run([optimizer, cost], feed_dict={X1:input_data1, X2:input_data2, Y:batch_ys, batch_prob : True})\n",
    "        total_cost += cost_val\n",
    "    saver.save(sess, checkpoint_path, global_step=epoch+1)\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'Avg.cost=', '{:.3f}'.format(total_cost / total_batch))\n",
    "    if (epoch+1)%5 == 0:\n",
    "        test_size=100\n",
    "        is_correct=tf.equal(tf.argmax(tf.nn.softmax(model),1),tf.argmax(Y,1))\n",
    "        accuracy=tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "        test_img1 = np.empty((test_size, data_x_size, data_y_size,3))\n",
    "        test_img2 = np.empty((test_size, data_x_size, data_y_size,3))\n",
    "        for i in range(test_size):\n",
    "            test_img1[i,:,:,:] = cv2.resize(cv2.imread(test_dir+'/'+test_list[test_data1[test_data_index[i]]],cv2.IMREAD_COLOR),(data_y_size,data_x_size))\n",
    "            test_img2[i,:,:,:] = cv2.resize(cv2.imread(test_dir+'/'+test_list[test_data2[test_data_index[i]]],cv2.IMREAD_COLOR),(data_y_size,data_x_size))\n",
    "\n",
    "        ac = sess.run(accuracy, feed_dict={X1:test_img1,\n",
    "                                            X2:test_img2,\n",
    "                                            Y:test_CoN[test_data_index[:test_size]],\n",
    "                                            batch_prob : False})\n",
    "        print('정확도:', ac)\n",
    "\n",
    "\n",
    "    \n",
    "print('Optimized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 저장된 모델과 파라미터를 불러옵니다.\n",
    "# SAVER_DIR 위치에 저장된 모델과 파라미터를 불러옵니다.\n",
    "\n",
    "try:\n",
    "    sess.close()\n",
    "    print(\"closing session\")\n",
    "except:\n",
    "    print(\"no session\")\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "SAVER_DIR = \"Re_Id_TEST/New/sp3_bot\"\n",
    "checkpoint_path = os.path.join(SAVER_DIR, \"model\")\n",
    "ckpt = tf.train.get_checkpoint_state(SAVER_DIR)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "new_saver = tf.train.import_meta_graph(ckpt.model_checkpoint_path+\".meta\")\n",
    "new_saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "tf.get_default_graph()\n",
    "\n",
    "X1 = sess.graph.get_tensor_by_name(\"input1:0\")\n",
    "X2 = sess.graph.get_tensor_by_name(\"input2:0\")\n",
    "batch_prob = sess.graph.get_tensor_by_name(\"batch_prob:0\")\n",
    "model = sess.graph.get_tensor_by_name(\"Dense/model:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 인식률을 테스트 하기 위해 데이터를 뽑습니다. 경로를 잘 확인해 주세요\n",
    "\n",
    "# temp_dir = '/home/esdl/tensorflow/campus'\n",
    "# temp_dir = '/home/esdl/tensorflow/cuhk03_release/images/detected'\n",
    "temp_dir = '/home/esdl/tensorflow/Market-1501-v15.09.15/bounding_box_train'\n",
    "pick_person = 100\n",
    "\n",
    "temp_list,temp1,temp2,temp_CoN = ALL_Image_Data(temp_dir,pick_person)\n",
    "temp_index = list(range(len(temp1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트가 다 진행되고 나면 Rank 인식률을 확인할 수 있습니다. 그림을 통해 좀더 알기 쉽게 표현해 줍니다.\n",
    "\n",
    "display = True\n",
    "\n",
    "data_x_size = 160\n",
    "data_y_size = 60\n",
    "\n",
    "start_point = 0\n",
    "\n",
    "batch_limit = 500\n",
    "batch = [0, 0, 0] # start, end, length\n",
    "total_batch = 0\n",
    "change = temp1[0]\n",
    "order = 1\n",
    "\n",
    "rank = []\n",
    "rank_index = []\n",
    "total_result = []\n",
    "rank_accuracy = np.empty((pick_person,20),dtype=int)\n",
    "\n",
    "for i in range(len(temp1)):\n",
    "    \n",
    "    if change == temp1[i] and i+1 != len(temp1) and (i-batch[0]) < batch_limit:\n",
    "        continue\n",
    "        \n",
    "    change = temp1[i]\n",
    "    batch[1] = i\n",
    "    batch[2] = batch[1]-batch[0]\n",
    "    \n",
    "    if i+1 == len(temp1):\n",
    "        batch[1] += 1\n",
    "        batch[2] += 1\n",
    "        \n",
    "    img1 = np.empty((batch[2], data_x_size, data_y_size, 3),dtype='uint8')\n",
    "    img2 = np.empty((batch[2], data_x_size, data_y_size, 3),dtype='uint8')\n",
    "    result = np.empty(batch[2],dtype=int)\n",
    "       \n",
    "    for j in range(0,batch[2]):\n",
    "        img1[j,:,:,:] = cv2.resize(cv2.imread(temp_dir+'/'+temp_list[temp1[batch[0]+j]],cv2.IMREAD_COLOR),(data_y_size,data_x_size))\n",
    "        img2[j,:,:,:] = cv2.resize(cv2.imread(temp_dir+'/'+temp_list[temp2[batch[0]+j]],cv2.IMREAD_COLOR),(data_y_size,data_x_size))\n",
    "        result[j] = temp_CoN[batch[0]+j,1]\n",
    "    batch[0] = i\n",
    "    \n",
    "    answer = tf.nn.softmax(model)\n",
    "    #answer = tf.transpose(answer,[1, 0, 2])\n",
    "    #answer= tf.reduce_mean(answer, axis=1)\n",
    "    answer = sess.run(answer,feed_dict={X1:img1, X2:img2, batch_prob : False})\n",
    "    rank.extend(answer[:,1])\n",
    "    total_result.extend(result)\n",
    "    \n",
    "    if temp1[i] == temp1[i-1] and i+1 != len(temp1): continue\n",
    "    \n",
    "    rank = np.array(rank)\n",
    "    total_result = np.array(total_result)\n",
    "    rank_index = np.argsort(rank)\n",
    "    ranking = np.round((rank[rank_index[::-1]]*100),4)\n",
    "    \n",
    "    print(\"PID :\",order)\n",
    "    \n",
    "    total_result = total_result[rank_index[::-1]][:20]\n",
    "    rank_accuracy[order-1,:] = total_result\n",
    "    print(total_result)\n",
    "    \n",
    "    order += 1\n",
    "    \n",
    "    if display:\n",
    "        fig , ax = plt.subplots(1, 14, figsize=(15,15))\n",
    "        ax[0].set_axis_off()\n",
    "        ax[0].imshow(cv2.cvtColor(img1[0], cv2.COLOR_BGR2RGB))\n",
    "        ax[0].set_title(\"anchor\")\n",
    "        for j in range(1,14):\n",
    "            ax[j].set_axis_off()\n",
    "            compare_image = cv2.resize(cv2.imread(temp_dir+'/'+temp_list[temp2[start_point+rank_index[-j]]],cv2.IMREAD_COLOR),\n",
    "                                                                                       (data_y_size,data_x_size))\n",
    "            #ax[j].imshow(cv2.cvtColor(img2[rank_index[-j]], cv2.COLOR_BGR2RGB))\n",
    "            ax[j].imshow(cv2.cvtColor(compare_image, cv2.COLOR_BGR2RGB))\n",
    "            ax[j].set_title(ranking[j])\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "    \n",
    "    rank = []\n",
    "    rank_index = []\n",
    "    total_result = []\n",
    "    start_point = i\n",
    "\n",
    "print(\"rank = 1 accuarcy is :\",rank_accuracy[:,0].sum(axis=0))\n",
    "print(\"rank = 5 accuarcy is :\",(rank_accuracy[:,:5].sum(axis=1)>0).astype(int).sum(axis=0))\n",
    "print(\"rank = 10 accuarcy is :\",(rank_accuracy[:,:10].sum(axis=1)>0).astype(int).sum(axis=0))\n",
    "print(\"rank = 20 accuarcy is :\",(rank_accuracy[:,:20].sum(axis=1)>0).astype(int).sum(axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
